# For more information, see the Configuration Guide:
# https://www.librechat.ai/docs/configuration/librechat_yaml

# Configuration version (required)
version: 1.2.1

# Cache settings: Set to true to enable caching
cache: true

# File strategy s3/firebase
# fileStrategy: "s3"

# Custom interface configuration
interface:
  customWelcome: "Welcome to Butler GPT!"
  # Privacy policy settings

  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true
  prompts: true
  bookmarks: true
  multiConvo: true
  agents: true
  runCode: false

# Example Registration Object Structure (optional)
registration:
  socialLogins: ['github', 'google']
  # allowedDomains:
  # - "gmail.com"


# Example Balance settings
# balance:
#   enabled: false
#   startBalance: 20000
#   autoRefillEnabled: false
#   refillIntervalValue: 30
#   refillIntervalUnit: 'days'
#   refillAmount: 10000

# speech:
#   tts:
#     openai:
#       url: ''
#       apiKey: '${TTS_API_KEY}'
#       model: ''
#       voices: ['']

#
#   stt:
#     openai:
#       url: ''
#       apiKey: '${STT_API_KEY}'
#       model: ''

# rateLimits:
#   fileUploads:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for file uploads per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for file uploads per user
#   conversationsImport:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for conversation imports per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for conversation imports per user

# Example Actions Object Structure
#actions:
#  allowedDomains:
#    - "swapi.dev"
#    - "librechat.ai"
#    - "google.com"

# Example MCP Servers Object Structure
# mcpServers:
#   everything:
#     # type: sse # type can optionally be omitted
#     url: http://localhost:3001/sse
#     timeout: 60000  # 1 minute timeout for this server, this is the default timeout for MCP servers.
#   puppeteer:
#     type: stdio
#     command: npx
#     args:
#       - -y
#       - "@modelcontextprotocol/server-puppeteer"
#     timeout: 300000  # 5 minutes timeout for this server
#   filesystem:
#     # type: stdio
#     command: npx
#     args:
#       - -y
#       - "@modelcontextprotocol/server-filesystem"
#       - /home/user/LibreChat/
#     iconPath: /home/user/LibreChat/client/public/assets/logo.svg
#   mcp-obsidian:
#     command: npx
#     args:
#       - -y
#       - "mcp-obsidian"
#       - /path/to/obsidian/vault

# Definition of custom endpoints
endpoints:
  # assistants:
  #   disableBuilder: false # Disable Assistants Builder Interface by setting to `true`
  #   pollIntervalMs: 3000  # Polling interval for checking assistant updates
  #   timeoutMs: 180000  # Timeout for assistant operations
  #   # Should only be one or the other, either `supportedIds` or `excludedIds`
  #   supportedIds: ["asst_supportedAssistantId1", "asst_supportedAssistantId2"]
  #   # excludedIds: ["asst_excludedAssistantId"]
  #   # Only show assistants that the user created or that were created externally (e.g. in Assistants playground).
  #   # privateAssistants: false # Does not work with `supportedIds` or `excludedIds`
  #   # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature
  #   retrievalModels: ["gpt-4-turbo-preview"]
  #   # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["code_interpreter", "retrieval", "actions", "tools", "image_vision"]
  # agents:
  #   # (optional) Default recursion depth for agents, defaults to 25
  #   recursionLimit: 50
  #   # (optional) Max recursion depth for agents, defaults to 25
  #   maxRecursionLimit: 100
  #   # (optional) Disable the builder interface for agents
  #   disableBuilder: false
  #   # (optional) Agent Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["execute_code", "file_search", "actions", "tools"]
  custom:
    - name: 'local'
      apiKey: 'a'
      baseURL: 'http://10.10.199.29:8080/v1'
      models:
        default:
          [
            'qwen3-30b-a3b-mlx',
          ]
        fetch: true
      titleConvo: true
      titleModel: "qwen3-30b-a3b-nothink-mlx"
      summarize: false
      summaryModel: "current_model"
      modelDisplayLabel: 'local'

    # OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: '${OPENROUTER_KEY}'
      baseURL: 'https://openrouter.ai/api/v1'
      models:
        default: ['meta-llama/llama-3-70b-instruct']
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'
# fileConfig:
#   endpoints:
#     assistants:
#       fileLimit: 5
#       fileSizeLimit: 10  # Maximum size for an individual file in MB
#       totalSizeLimit: 50  # Maximum total size for all files in a single request in MB
#       supportedMimeTypes:
#         - "image/.*"
#         - "application/pdf"
#     openAI:
#       disabled: true  # Disables file uploading to the OpenAI endpoint
#     default:
#       totalSizeLimit: 20
#     YourCustomEndpointName:
#       fileLimit: 2
#       fileSizeLimit: 5
#   serverFileSizeLimit: 100  # Global server file size limit in MB
#   avatarSizeLimit: 2  # Limit for user avatar image size in MB
# # See the Custom Configuration Guide for more information on Assistants Config:
# # https://www.librechat.ai/docs/configuration/librechat_yaml/object_structure/assistants_endpoint
